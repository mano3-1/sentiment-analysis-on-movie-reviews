{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Importing all dependencies "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from transformers import *\nimport tokenizers\nimport numpy as np\nimport zipfile\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import StratifiedKFold\nimport math\nimport pickle","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## This is to create a strategy from tpu"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create strategy from tpu\nAUTO = tf.data.experimental.AUTOTUNE\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\nstrategy = tf.distribute.experimental.TPUStrategy(tpu)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading tokenizers and roberta\n### The weights files, vocab and merge files are available at [huggingface transformers](https://huggingface.co/transformers/_modules/transformers/modeling_tf_roberta.html#TFRobertaModel)"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"tokenizer = tokenizers.ByteLevelBPETokenizer(\n    vocab_file='../input/tf-roberta/vocab-roberta-base.json', \n    merges_file='../input/tf-roberta/merges-roberta-base.txt', \n    lowercase=True,\n    add_prefix_space=True\n)\nconfig = RobertaConfig.from_pretrained('../input/tf-roberta/config-roberta-base.json')\nbert_model = TFRobertaModel.from_pretrained('../input/tf-roberta/pretrained-roberta-base.h5',config=config)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Reading the training csv with pandas.read_csv"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_csv = pd.read_csv('../input/sentiment-analysis-on-movie-reviews/train.tsv.zip' ,sep = '\\t')\ntrain_csv.head(50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## These are the parameters that I used for the experiments, these can be adjusted to improve the performance"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_csv.Sentiment.value_counts())\nPAD_ID = 0\nMAX_LEN = 80\nEPOCHS = 5\nDISPLAY = 1\nBATCH_SIZE = 8 * strategy.num_replicas_in_sync\nLABEL_SMOOTHING = 0.1\nprint(BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_csv.shape[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tokenizing the dataset with roberta tokenizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"ct = train_csv.shape[0]\ninput_ids = np.zeros((ct,MAX_LEN),dtype='int32')\nattention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\nsentiments =  np.zeros((ct ,5))\nfor k in range(train_csv.shape[0]):\n    \n    # FIND OVERLAP\n    text1 = \" \"+\" \".join(train_csv.loc[k,'Phrase'].split())\n    enc = tokenizer.encode(text1) \n    target = train_csv.loc[k ,'Sentiment']\n    sentiments[k][target] = 1\n    input_ids[k,:len(enc.ids)+2] = [0] + enc.ids + [2]\n    attention_mask[k ,:len(enc.ids)+2] = 1\n    if k == 2:\n        print(text1)\n        print(input_ids[k])\n        man = tokenizer.encode(text1)\n        print(target)\n        print(sentiments[k])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tokenizing test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/sentiment-analysis-on-movie-reviews/test.tsv.zip' ,sep = '\\t')\n\nct = test.shape[0]\ninput_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\nattention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\nfor k in range(test.shape[0]):\n        \n    # INPUT_IDS\n    text1 = \" \"+\" \".join(test.loc[k,'Phrase'].split())\n    enc = tokenizer.encode(text1)              \n    attention_mask_t[k ,:len(enc.ids)+2] = 1\n    input_ids_t[k,:len(enc.ids)+2] = [0] + enc.ids + [2]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Building a CNN head, focal loss and some helper functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def save_weights(model, dst_fn):\n    weights = model.get_weights()\n    with open(dst_fn, 'wb') as f:\n        pickle.dump(weights, f)\n\n\ndef load_weights(model, weight_fn):\n    with open(weight_fn, 'rb') as f:\n        weights = pickle.load(f)\n    model.set_weights(weights)\n    return model\ndef focal_loss(alpha ,gamma):\n    def loss_fn(y_true, y_pred):\n    # adjust the targets for sequence bucketing\n        y_pred = K.clip(y_pred ,1e-7 ,1-1e-7)\n        loss = -alpha*((1-y_pred)**gamma)*y_true*K.log(y_pred)\n        loss = K.sum(loss ,axis = -1)\n        return loss\n    return loss_fn\ndef scheduler(epoch):\n    return 3e-5 * 1.15**epoch\n\ndef build_model(alpha ,gamma):\n\n    config = RobertaConfig.from_pretrained('../input/tf-roberta/config-roberta-base.json')\n    bert_model = TFRobertaModel.from_pretrained('../input/tf-roberta/pretrained-roberta-base.h5',config=config)\n    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    x = bert_model(ids)\n    #x1 = tf.keras.layers.Dropout(0.1)(x[0])\n    x1 = tf.keras.layers.Conv1D(2048, 2,padding='same')(x[0])\n    x1 = tf.keras.layers.LeakyReLU()(x1)\n    #x1 = tf.keras.layers.Dropout(0.1)(x1)\n    x1 = tf.keras.layers.Conv1D(1024, 2)(x1)\n    x1 = tf.keras.layers.LeakyReLU()(x1)\n    #x1 = tf.keras.layers.Dropout(0.1)(x1)\n    x1 = tf.keras.layers.Conv1D(512, 2,padding='same')(x1)\n    x1 = tf.keras.layers.LeakyReLU()(x1)\n    #x1 = tf.keras.layers.Dropout(0.1)(x1)\n    x1 = tf.keras.layers.Conv1D(256, 2)(x1)\n    x1 = tf.keras.layers.LeakyReLU()(x1)\n    #x1 = tf.keras.layers.Dropout(0.1)(x1)\n    x1 = tf.keras.layers.Conv1D(128, 2,padding='same')(x1)\n    x1 = tf.keras.layers.LeakyReLU()(x1)\n    x1 = tf.keras.layers.Dense(1)(x1)\n    x1 = tf.keras.layers.Flatten()(x1)\n    x1 = tf.keras.layers.Dense(5)(x1)\n    x1 = tf.keras.layers.Activation('softmax')(x1)\n    \n\n    model = tf.keras.models.Model(inputs=[ids ,att], outputs=[x1])\n    optimizer = tf.keras.optimizers.Adam(learning_rate=0.00001) \n    model.compile(loss=focal_loss(alpha ,gamma), optimizer=optimizer ,metrics = ['acc'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Using K-Fold cross-validation training startegy"},{"metadata":{"trusted":true},"cell_type":"code","source":"jac = []; VER='v0'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\noof = np.zeros((input_ids.shape[0],5))\npreds = np.zeros((input_ids_t.shape[0],5))\n\nskf = StratifiedKFold(n_splits=6,shuffle=True) #,random_state=SEED) #originally 5 splits\nfor fold,(idxT,idxV) in enumerate(skf.split(input_ids,train_csv.Sentiment)):\n\n    print('#'*25)\n    print('### FOLD %i'%(fold+1))\n    print('#'*25)\n    \n    K.clear_session()\n    with strategy.scope():\n        model = build_model(2.0 ,1.5)\n        \n    #sv = tf.keras.callbacks.ModelCheckpoint(\n    #    '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n    #    save_weights_only=True, mode='auto', save_freq='epoch')\n    inpT = [input_ids[idxT ,] ,attention_mask[idxT ,]]\n    targetT = [sentiments[idxT ,]]\n    inpV = [input_ids[idxV,] ,attention_mask[idxV ,]]\n    targetV = [sentiments[idxV ,]]\n    train_dataset = (tf.data.Dataset\n    .from_tensor_slices(({'input1' : inpT[0],'input2':inpT[1]}, targetT[0]))\n    .repeat()\n    .shuffle(2048)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\n    valid_dataset = (tf.data.Dataset\n    .from_tensor_slices(({'input1' : inpV[0],'input2':inpV[1]}, targetV[0]))\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO)\n)\n\n    test_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(({'input1' : input_ids_t,'input2': attention_mask_t}))\n    .batch(BATCH_SIZE)\n)\n    n_steps = len(idxT)//BATCH_SIZE\n    # sort the validation data\n    reduce_lr = tf.keras.callbacks.LearningRateScheduler(scheduler)\n    model.fit(inpT ,targetT[0] ,epochs = EPOCHS ,steps_per_epoch = n_steps ,verbose = DISPLAY ,callbacks = [reduce_lr] ,validation_data = (inpV ,targetV[0]))\n    print('Loading model...')\n    # model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n    weight_fn = 'v0-roberta-{}.h5'.format(fold)\n    save_weights(model ,weight_fn)\n    load_weights(model, weight_fn)\n\n    print('Predicting OOF...')\n    oof[idxV,] = model.predict(inpV ,targetV[0],verbose=DISPLAY)\n    \n    print('Predicting Test...')\n    preds_ = model.predict([input_ids_t ,attention_mask_t],verbose=DISPLAY)\n    preds += preds_/skf.n_splits\n    mypreds = []\n    GT = [] \n    for k in idxV:\n        mypreds.append(np.argmax(oof[k,]))\n        GT.append(np.argmax(sentiments[k,]))\n    print('f1 score : {}'.format(f1_score(GT ,mypreds ,average = 'weighted')))\n    print('acc score : {}'.format(accuracy_score(GT ,mypreds)))\n    print('cm : {}'.format(confusion_matrix(GT ,mypreds)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}